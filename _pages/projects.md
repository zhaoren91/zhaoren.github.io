---
layout: archive
title: "Projects"
permalink: /projects/
author_profile: false
---
{% include base_path %}

<a href="" target="_blank"></a>
# MyVoice: Myoelectric Vocal Interaction and Communication Engine (funded by DFG)
*Runtime*: 2022 - 2025

*Abstract*: In this project, Dr. Ren extends her research from audio and speech to biosignals, especially the Electromyography (EMG) signals. The goal is to synthesise speech from EMG signals, thereby helping patients who cannot produce acoustic speech. 


<a href="" target="_blank"></a>
# <a href="https://leibniz-ai-lab.de/" target="_blank">Leibniz AI Lab</a>: International Leibniz Future Laboratory for Artificial Intelligence
*Runtime*: 2020 - 2023

*Role*: Project coordinator

*Partners*: Swinburne University of Technology, Stanford University, Nanyang Technological University, Indian Institute of Technology Kharagpur, University of Technology Sydney, Victoria University of Wellington, ADSC Illinois At Singapore Pte. Ltd., Edith Cowan University, Leibniz Universität Hannover, Medizinische Hochschule Hannover, Zentrum für Individualisierte Infektionsmedizin, Leibniz Alliance Hannover

*Abstract*: The Leibniz Future Lab, funded by the Federal Ministry of Education and Research, bring together researchers from all over the world to work on the future of artificial intelligence (AI). The Leibniz Future Lab will focus on state-of-the-art research in artificial intelligence and also develop intelligent solutions for personalised medicine, which is supposed to achieve individual therapies, medication tailored to the individual patient and more precise diagnoses.


<a href="" target="_blank"></a>
# AUDI0NOMOUS: Agent-based Unsupervised Deep Interactive 0-shot-learning Networks Optimising Machines’ Ontological Understanding of Sound <br>DFG (German Research Foundation) Reinhart Koselleck-Projekt (No. 442218748)
*Runtime*: 01.01.2021 - 31.12.2025

*Role*: Participant

*Abstract*: Soundscapes are a component of our everyday acoustic environment. AUDI0NOMOUS aims to achieve major breakthroughs in analysis, categorisation, and understanding of real-life soundscapes. A novel approach based on the development of four highly cooperative and interactive intelligent agents is proposed. A Curious Agent will collect unique data from web sources/social media; an Audio Decomposition Agent will decompose overlapped sounds; a Learning Agent will recognise an unlimited number of unlabelled sound; and, an Ontology Agent will translate the soundscapes into verbal ontologies. AUDI0NOMOUS will open up an entirely new dimension of comprehensive audio understanding, and promote advancements in health care, robotics, and smart devices and cities, amongst many others.

# <a href="https://www.tapas-etn-eu.org/" target="_blank">TAPAS</a>: Training Network on Automatic Processing of PAthological Speech (No. 766287)
*Runtime*: 01.11.2017 - 31.10.2021

*Role*: Participant

*Partners*: Idiap Research Institute, Friedrich-Alexander-Univeristät Erlangen Nuernberg, imec - Ghent University, INESC ID - Instituto de Engenhariade Sistemas E Computabores Investigacao E Desenvolvimento em Lisboa, Ludwig-Maximilians-Universität Muenchen, The Netherlands Cancer Institute - Antoni van Leeuwenhoek, Philips Research Eindhoven, Radboud Universiteit, Nijmegen, University of Augsburg, Université Toulouse III-Paul Sabatier, The University of Sheffield, Antwerp University Hospital, Associazione Italiana Assistenza Agli Spastici Provincia di Bologna, audEERING GmbH, Centre Hospitalier Universitaire de Toulouse, EML European Media Laboratory GmbH, Heidelberg, Institut der Kasseler Stottertherapie, Medizinische Universitaet Wien, Oxford Wave Research, Therapy Box Limited, University Medical Center of Utrecht (UMC Utrecht), University of Antioquia

*Abstract*: A number of people across Europe with debilitating speech pathologies are facing communation problems. With developing speech processing techniques, the aim of the project is to address the three research problems: i) detecting conditions that impact on speech production, ii) producing automated speech therapy tools, and iii) re-designing speech technology to make it works well for people with speech impairments and helps in making informed clinical choices.

# <a href="http://www.emotass.de/de/" target="_blank">EmotAsS</a>: Assistance system for recognising the emotional state of workshop employees (No. 16SV7213)
*Runtime*: 01.06.2015 - 31.05.2018

*Role*: Participant

*Partners*: University of Bremen, University of Augsburg, University of Passau, vacances Mobiler Sozial- und Pflegedienst GmbH, Martinshof (Werkstatt Bremen), Meier und Schütte GmbH und Co. KG.
*Abstract*: This project aims to develop an emotion-sensitive, voice-controlled assistance system, which can reliably recognise the emotions of people with disabilities in manufactories from their interactions with the voice assistant. The developed system should reliably recognise, respond, and react to emotions of people with disabilities during their everyday work routinge.

# <a href="http://www.ihearu.eu/" target="_blank">iHEARu</a>: Intelligent systems' Holistic Evolving Analysis of Real-life Universal speaker characteristics (No. 338164)
*Runtime*: 01.01.2014 - 31.12.2018

*Role*: Participant

*Partners*: University of Augsburg, University of Passau, TUM

*Abstract*: The aim of this project is to promote intelligent systems for computational paralinguistics by holistic processing multiple speaker attributes at once, evolving and self-learning, deeper analysing acoustic parameters on realistic data, ultimately progressing from individual analysis tasks towards universal speaker characteristics analysis, which can easily learn about and can be adapted to new characteristics.
